import Foundation

//
// MaliciousContentDetector.swift
// SignalAir
//
// æƒ¡æ„å…§å®¹æª¢æ¸¬å’Œåˆ†é¡žç³»çµ±
//

/// æƒ¡æ„å…§å®¹é¡žåž‹å®šç¾©
enum MaliciousContentType: String, CaseIterable {
    case spam = "spam"
    case harassment = "harassment"
    case hateSpeech = "hate_speech"
    case threats = "threats"
    case phishing = "phishing"
    case adultContent = "adult_content"
    case misinformation = "misinformation"
    case botGenerated = "bot_generated"
    
    var severity: MaliciousContentSeverity {
        switch self {
        case .spam:
            return .low
        case .harassment, .misinformation:
            return .medium
        case .hateSpeech, .phishing, .adultContent:
            return .high
        case .threats, .botGenerated:
            return .critical
        }
    }
    
    var description: String {
        switch self {
        case .spam:
            return "åžƒåœ¾è¨Šæ¯"
        case .harassment:
            return "é¨·æ“¾å…§å®¹"
        case .hateSpeech:
            return "ä»‡æ¨è¨€è«–"
        case .threats:
            return "å¨è„…æåš‡"
        case .phishing:
            return "ç¶²è·¯é‡£é­š"
        case .adultContent:
            return "æˆäººå…§å®¹"
        case .misinformation:
            return "éŒ¯èª¤è¨Šæ¯"
        case .botGenerated:
            return "æ©Ÿå™¨äººå…§å®¹"
        }
    }
}

/// æƒ¡æ„å…§å®¹åš´é‡æ€§ç­‰ç´š
enum MaliciousContentSeverity: Int, CaseIterable {
    case low = 1
    case medium = 2
    case high = 3
    case critical = 4
    
    var trustScorePenalty: Double {
        switch self {
        case .low: return 5.0
        case .medium: return 10.0
        case .high: return 15.0
        case .critical: return 25.0
        }
    }
}

/// å…§å®¹åˆ†æžçµæžœ
struct ContentAnalysisResult {
    let isClean: Bool
    let detectedTypes: [MaliciousContentType]
    let confidence: Double // 0.0 - 1.0
    let details: String
    
    var maxSeverity: MaliciousContentSeverity {
        return detectedTypes.map { $0.severity }.max() ?? .low
    }
}

/// æƒ¡æ„å…§å®¹æª¢æ¸¬å™¨
class MaliciousContentDetector {
    
    // MARK: - æ“´å±•çš„ç¦ç”¨è©žå½™åº«
    
    private let spamKeywords = [
        "spam", "åžƒåœ¾", "å»£å‘Š", "ä¿ƒéŠ·", "å„ªæƒ ", "å…è²»", "è³ºéŒ¢",
        "æŠ•è³‡", "ç†è²¡", "è²¸æ¬¾", "ä¿¡ç”¨å¡", "ä¸­çŽ", "æŠ½çŽ"
    ]
    
    private let harassmentKeywords = [
        "æ­»", "æ®º", "ç¬¨è›‹", "ç™½ç—´", "æ™ºéšœ", "å»¢ç‰©", "åžƒåœ¾äºº",
        "get out", "stupid", "idiot", "loser", "freak"
    ]
    
    private let hateSpeechKeywords = [
        "ç¨®æ—", "æ­§è¦–", "ä»‡æ¨", "æŽ’æ–¥", "åŠ£ç­‰", "å„ªç­‰",
        "racist", "discrimination", "hate", "inferior", "superior"
    ]
    
    private let threatKeywords = [
        "å¨è„…", "æåš‡", "å ±å¾©", "å‚·å®³", "æ®ºå®³", "æ¯€æŽ‰",
        "threat", "kill", "destroy", "harm", "revenge", "attack"
    ]
    
    private let phishingKeywords = [
        "é»žæ“Šé€£çµ", "è¼¸å…¥å¯†ç¢¼", "ç·Šæ€¥é©—è­‰", "å¸³è™Ÿç•°å¸¸", "ç«‹å³è™•ç†",
        "click here", "verify account", "urgent", "suspended", "login"
    ]
    
    private let adultKeywords = [
        // åŸºæœ¬æˆäººå…§å®¹é—œéµå­—ï¼ˆå¯ä¾éœ€è¦æ“´å±•ï¼‰
        "è‰²æƒ…", "æ€§æ„›", "è£¸é«”", "æˆäºº", "18+",
        "porn", "sex", "nude", "adult", "xxx"
    ]
    
    // MARK: - ä¸»è¦æª¢æ¸¬æ–¹æ³•
    
    /// åˆ†æžæ–‡å­—å…§å®¹æ˜¯å¦åŒ…å«æƒ¡æ„å…§å®¹
    func analyzeContent(_ text: String) -> ContentAnalysisResult {
        let cleanText = text.lowercased().trimmingCharacters(in: .whitespacesAndNewlines)
        var detectedTypes: [MaliciousContentType] = []
        var confidence: Double = 0.0
        
        // æª¢æ¸¬åžƒåœ¾è¨Šæ¯
        if containsKeywords(cleanText, from: spamKeywords) {
            detectedTypes.append(.spam)
            confidence = max(confidence, 0.7)
        }
        
        // æª¢æ¸¬é¨·æ“¾å…§å®¹
        if containsKeywords(cleanText, from: harassmentKeywords) {
            detectedTypes.append(.harassment)
            confidence = max(confidence, 0.8)
        }
        
        // æª¢æ¸¬ä»‡æ¨è¨€è«–
        if containsKeywords(cleanText, from: hateSpeechKeywords) {
            detectedTypes.append(.hateSpeech)
            confidence = max(confidence, 0.9)
        }
        
        // æª¢æ¸¬å¨è„…å…§å®¹
        if containsKeywords(cleanText, from: threatKeywords) {
            detectedTypes.append(.threats)
            confidence = max(confidence, 0.95)
        }
        
        // æª¢æ¸¬ç¶²è·¯é‡£é­š
        if containsKeywords(cleanText, from: phishingKeywords) {
            detectedTypes.append(.phishing)
            confidence = max(confidence, 0.85)
        }
        
        // æª¢æ¸¬æˆäººå…§å®¹
        if containsKeywords(cleanText, from: adultKeywords) {
            detectedTypes.append(.adultContent)
            confidence = max(confidence, 0.8)
        }
        
        // æª¢æ¸¬æ©Ÿå™¨äººç”Ÿæˆå…§å®¹ï¼ˆåŸºæ–¼æ¨¡å¼ï¼‰
        if detectBotPattern(cleanText) {
            detectedTypes.append(.botGenerated)
            confidence = max(confidence, 0.75)
        }
        
        let details = detectedTypes.isEmpty ? "å…§å®¹æ¸…æ½”" : "æª¢æ¸¬åˆ°: \(detectedTypes.map { $0.description }.joined(separator: ", "))"
        
        return ContentAnalysisResult(
            isClean: detectedTypes.isEmpty,
            detectedTypes: detectedTypes,
            confidence: confidence,
            details: details
        )
    }
    
    // MARK: - è¼”åŠ©æª¢æ¸¬æ–¹æ³•
    
    private func containsKeywords(_ text: String, from keywords: [String]) -> Bool {
        return keywords.contains { keyword in
            text.contains(keyword)
        }
    }
    
    private func detectBotPattern(_ text: String) -> Bool {
        // æª¢æ¸¬æ©Ÿå™¨äººæ¨¡å¼ï¼š
        // 1. éŽåº¦é‡è¤‡çš„å­—ç¬¦
        let repeatingPattern = try? NSRegularExpression(pattern: "(.{1,3})\\1{4,}", options: [])
        if let regex = repeatingPattern,
           regex.firstMatch(in: text, range: NSRange(text.startIndex..., in: text)) != nil {
            return true
        }
        
        // 2. ç•°å¸¸çš„å¤§å¯«æ¨¡å¼
        let uppercaseRatio = Double(text.filter { $0.isUppercase }.count) / Double(text.count)
        if uppercaseRatio > 0.8 && text.count > 10 {
            return true
        }
        
        // 3. éŽå¤šçš„ç‰¹æ®Šå­—ç¬¦
        let specialCharRatio = Double(text.filter { !$0.isLetter && !$0.isNumber && !$0.isWhitespace }.count) / Double(text.count)
        if specialCharRatio > 0.5 && text.count > 5 {
            return true
        }
        
        return false
    }
    
    /// å¿«é€Ÿæª¢æŸ¥æ˜¯å¦ç‚ºæ˜Žé¡¯æƒ¡æ„å…§å®¹
    func isObviouslyMalicious(_ text: String) -> Bool {
        let result = analyzeContent(text)
        return !result.isClean && result.confidence > 0.8
    }
    
    /// å–å¾—å»ºè­°çš„è™•ç†å‹•ä½œ
    func getRecommendedAction(for result: ContentAnalysisResult) -> ContentModerationAction {
        if result.isClean {
            return .allow
        }
        
        switch result.maxSeverity {
        case .low:
            return .warn
        case .medium:
            return .filter
        case .high:
            return .block
        case .critical:
            return .blockAndBan
        }
    }
}

/// å…§å®¹å¯©æ ¸å‹•ä½œ
enum ContentModerationAction {
    case allow          // å…è¨±
    case warn           // è­¦å‘Š
    case filter         // éŽæ¿¾
    case block          // é˜»æ“‹
    case blockAndBan    // é˜»æ“‹ä¸¦å°ç¦
}

/// æ“´å±•ï¼šèˆ‡ç¾æœ‰ä¿¡ä»»åˆ†æ•¸ç³»çµ±æ•´åˆ
extension MaliciousContentDetector {
    
    /// æª¢æ¸¬æƒ¡æ„å…§å®¹ä¸¦å›žå ±çµ¦ä¿¡ä»»åˆ†æ•¸ç³»çµ±
    func validateAndReport(_ text: String, from deviceUUID: String, trustManager: TrustScoreManager) -> Bool {
        let result = analyzeContent(text)
        
        if !result.isClean {
            // è§¸ç™¼ä¿¡ä»»åˆ†æ•¸æ‡²ç½°
            trustManager.recordSuspiciousBehavior(for: deviceUUID, behavior: .maliciousContent)
            
            // è¨˜éŒ„å®‰å…¨äº‹ä»¶
            let event = SecurityEvent(
                peerID: deviceUUID,
                type: .suspiciousActivity,
                severity: mapToSecuritySeverity(result.maxSeverity),
                details: "æƒ¡æ„å…§å®¹æª¢æ¸¬: \(result.details)",
                sourceComponent: "MaliciousContentDetector"
            )
            
            print("ðŸš¨ æª¢æ¸¬åˆ°æƒ¡æ„å…§å®¹: \(result.details) ä¾†è‡ª: \(deviceUUID)")
            return false
        }
        
        return true
    }
    
    private func mapToSecuritySeverity(_ contentSeverity: MaliciousContentSeverity) -> SecuritySeverity {
        switch contentSeverity {
        case .low: return .low
        case .medium: return .medium
        case .high: return .high
        case .critical: return .critical
        }
    }
}